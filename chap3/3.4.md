# 3.4 性能与伸缩性

本节将理论联系实际，看看现实世界中运行时是如何提升内存管理的执行性能的。

<a name="3.4.1" />
## 3.4.1 TLA（Thread local allocation）

**TLA（Thread Local Allocation）**可以用来加速对象的分配过程。一般情况下，如果能在线程局部缓冲区中为对象分配内存是肯定比直接在需要同步操作的堆上分配内存快的。垃圾回收器在堆上直接分配内存时是需要对整个堆加锁的，对于多线程竞争激烈的应用程序来说，这将会是一场灾难。因此，如果每个Java线程能够有一块线程内的局部缓冲区，那么绝大部分的对象分配操作就简单多了，在大多数硬件平台上，只需要一条汇编指令即可完成。这块线程局部缓冲区，就称为 **TLA（Thread Local Area）**。

为了更好的利用缓存，达到更高的性能，一般情况下，TLA的大小介于16KB到128KB之间，当然，也可以通过命令行参数显式指定。当TLA被填满时，垃圾回收器会将TLA中的内容"提升"到堆中。

当Java源代码中有`new`操作符，并且JIT编译器对内存分配执行高级优化之后，内存分配的伪代码如下所示：

    Object allocateNewObject(Class objectClass) {
        Thread current = getCurrentThread();
        int objectSize = alignedSize(objectClass);
        if (current.nextTLAOffset + objectSize > TLA_SIZE) {
            current.promoteTLAToHeap(); //slow and synchronized
            current.nextTLAOffset = 0;
        }
        Object ptr = current.TLAStart + current.nextTLAOffset;
        current.nextTLAOffset += objectSize;
        return ptr; 
    }

>为了说明内存分配问题，在上面的伪代码中省略了很多其他关联操作。例如，如果待分配的对象非常大，超过了某个阈值，或对象太大导致无法存放在TLA中，则会直接在堆中为对象分配内存。JRockit R28引入了 **TLA空闲限制（TLA waste limit）**来衡量TLA的使用情况，详细情况参见[第5章][1]内容]。

在某些包含大量寄存器的架构中，为了达到更高的性能，会将`nextTLAOffset`的值，有时还会将`current`的值，保存在当前线程自身的寄存器中，而对于像x86这种寄存器数量不多的架构来说，这样做就有些浪费了。

<a name="3.4.2" />
## 3.4.2 更到的堆内存

实现垃圾回收器的复杂度通常与存活对象集合的大小相关，与堆的大小没什么关系，因此，在存活对象集合不变的情况下，让垃圾回收器使用更大的堆内存没什么难度，这样做的好处是减缓了内存碎片化的趋势，并且可以存储更多的存活对象。

<a name="3.4.2.1" />
### 3.4.2.1 32位架构下的4GB内存限制

在32位系统中，内存的最大寻址范围是4GB。4GB是理论上可用的最大内存，但实际上，还有一些其他的东西会占用内存，例如操作系统自身会占用一些内存。而某些操作系统，对于内核和程序库在内存中的排布非常有严格要求，以Windows为例，它要求内核要位于内存地址空间的中段，这样就没办法将可用内存作为一个完整的连续空间使用。大部分JVM只能将连续内存空间作为堆使用，因此，这就大大限制了堆的大小。

就目前所知，JRockit是目前唯一支持以非连续内存空间作为堆使用的JVM，因此可以充分利用内存空间中被内核和其他程序库分隔开的内存。

![Figure 3-10 "Address Structure"][2]

在上图中，操作系统内核位于内存地址空间的中段，限制了进程的最大虚拟地址空间。内存区域 **A**和 **B**分别位于操作系统所处区域的两边，**A**比 **B**的空间稍大一些，因此，对于那些只能使用连续空间作为堆的虚拟机来说，**A**就是堆的最大容量。如果虚拟机能够有效率用非连续内存空间的话，就可以将区域 **A**和 **B**合并为一个整体使用，这种实现的前提假设是内核区域在程序运行时不会发生变动。

随着64位架构的出现，32位架构的使用正逐步减少，但还是有一些场景在大量使用32位架构，例如目前的虚拟化运行环境，在有限的内存中，针对32位平台做大量的性能优化还是很有意义的。

<a name="3.4.2.2" />
### 3.4.2.2 64位架构

在64位系统中，由于有了更大的虚拟地址空间可用（即使是运行32位JVM），开发人员无需再耗费精力去思考内存空间被非应用程序的内容占用的问题了。

大部分现代处理器已经是64位架构了，内存寻址范围理论上达到了16EB，这已经是非常惊人了，目前来看，装配如此巨大内存的机器的价格还太贵。

对于自动内存管理来说，64位架构是把双刃剑，有利有弊。在这点上，代码生成从64位架构中得到就全是利，例如更多的寄存器，位宽更大的寄存器和更大的数据带宽。

在64位机器上，指针的长度不再是4个字节，而是8个字节，这需要消耗更多的CPU缓存空间，简单来说就是，解引用一个32位指针的速度会比解引用64位指针快，所以，对于操作同样大小的堆来说，64位版本程序的运行速度会慢很多。

<a name="3.4.2.2.1" />
#### 3.4.2.2.1 压缩指针

针对前面提到的问题，一个折中的解决方案是 **压缩指针**，JRockit首先实现了这个优化策略。如果JVM运行在64位系统上，而且配置的堆小于4GB，很明显，这时候再使用64位指针是得不偿失的，32位指针已经够用，而且还可以加快对象的访问速度。

64位系统中，堆外的本地指针是系统运行时的一部分，仍然是64位的，例如[JNI][4]中用于引用Java对象的句柄。在代码中可能需要通过JNI转换，将压缩过的指针地址转换为普通的、系统宽度的地址，或是做反向转换，例如垃圾回收器和本地代码可能会在内部使用本地指针。这个转换过程就称为 **指针压缩（reference compression）**和 **指针解压缩（reference decompression）**。

下面的是64位平台上压缩/解压缩指针的伪代码，其中堆的基地址可能会在内存空间的任何位置：

    CompRef compress(Ref ref) {
        return (uint32_t)ref; //truncate reference to 32-bits
    }
    Ref decompress(CompRef ref) {
        return globalHeapBase | ref;
    }

压缩过的指针是32位的，通过与堆的基地址做`or`运算可以得出系统具有宽度的64位指针。这种操作的好处是解压缩操作不会改变指针的值，但是由于依赖于压缩指针的长度，所以虚拟机在使用的时候必须能够知晓某个指针是否是被压缩过的。

其实，压缩指针并不仅仅可以用于处理4GB堆的限制问题，还有用途。例如，现在堆的大小是64GB，在表示对象指针长度时，不需要使用64位，只用`4 + 32`位就可以了，具体方法是将64GB的堆划分为16个4GB的分区，然后用4位标明对象处于哪个分区中，再用32位标明对象的偏移地址。这种方式需要额外使用4位来标记对象的分区位置，导致对象的地址只能按16字节对齐，这回浪费一些堆空间，但在执行性能上会有所提升。

    CompRef compress(Ref ref) {
        return (uint32_t)(ref >> log2(objectAlignment)); 
    }
    Ref decompress(CompRef ref) {
        return globalHeapBase | (ref << log2(objectAlignment)); 
    }

其实，如果对象是以16字节对齐的，并且地址空间从0开始，到64GB结束的话，那么在具体实现指针压缩的时候还有更简单的方法。例如，解压缩指针时将指针的值左移4位即可，相对的，压缩指针时右移4位即可，无需使用堆的基地址参与计算。普通场景下，JRockit采用的就是这种实现方式来维护压缩过的、长度位32位指针。为了便于实现，JRcokit以地址0作为空指针的值，因此，堆中起始的4GB（即低地址方向的4GB）将无法使用，因此可用的堆空间实际上是60GB，虽然有些浪费，但相对于获得的性能提升，这不算什么。但如果对于应用程序来说，这4GB是不可或缺的话，那么就需要考虑使用其他方法了。
 
上述的实现方式适用于那些堆大于等于4GB的场景，但这种方式有一个缺点，压缩/解压缩的使用顺序和使用次数不再像以前一样不受限制了。

当然，64GB并不是理论上的限制值，只是举例而已，因为已经有基准测试和应用程序实例证明了，对于64GB大小的堆来说，启用指针压缩会获得更高的性能。其实，真正重要的是，启用压缩指针会浪费多少位，以及在此代价下，到底能获得多少性能方面的提升。在某些案例中，使用未经压缩的指针效果更好。

>在JRockit R28中，不同配置下，理论上，压缩指针可以支持最大64GB的堆，部分压缩指针框架可以自适应处理。
>
>如果启动JVM时没有指定堆的大小，或者指定的堆小于等于64GB，则会默认启用指针压缩的某个变种。因此，对象对齐的字节数取决于堆的大小。当然，也可以通过命令行参数显式的禁用指针压缩。
>
>译者注，参见下面文章的内容：
>
>[* Understanding Compressed References][5]

就JRockit来说，压缩指针的主要最大化可用堆内存的大小和可存放在L1缓存中的对象数量。为了避免在某些特殊场景下可能出现的问题，JRockit不会对局部栈帧中的指针进行压缩，一般来说，代码生成器会在载入对象域后插入解压缩指针的代码，在存储对象域之前插入压缩指针的代码。尽管这么做会有一些性能损耗，但非常小，几乎可以忽略不计。

<a name="3.4.3" />
## 3.4.3 Cache friendliness

It is also important for the garbage collector to care about other aspects of the underlying system architecture. The most important issue to consider is cache friendliness. Repeated cache misses can cause significant performance degradation.

>CPUs contain both instruction and data caches (along with several specialized caches for other purposes). In this section we are addressing data cache issues. A cache consists of several cache-lines of data, which is the smallest accessible cache unit. When data is retrieved from memory, it is placed into the caches so that it can be quickly accessed again in the near future. Accessing data in a cache is orders of magnitude faster than retrieving it from memory.
>
>A CPU cache is usually hierarchical and multi level (for example, with three levels). The first level of cache is the fastest, smallest, and closest to the CPU. On modern architectures, each core tends to have its own L1 cache, while higher level caches may or may not be shared between cores. An L1 cache is usually on the order of kilobytes and an L2 cache on the order of megabytes. Accessing an L2 cache is more expensive than an L1 cache, but still much cheaper than having to go to memory, which will be the case if the highest level cache in the hierarchy misses.
>
>Intelligent prefetching of data into a cache before it is to be accessed can both reduce the number of cache misses if it is the correct data, or destroy cache performance if cache contents are replaced by irrelevant data. An adaptive runtime seems like an ideal environment to find out what data is likely to be relevant.

In code generation, this can be remedied by using runtime feedback to determine which object accesses in Java code cause cache misses and compensate by generating code that does intelligent **prefetching**. In the memory system, we need to care about things like object placement on the heap, alignment, and allocation strategies.

<a name="3.4.3.1" />
### 3.4.3.1 Prefetching

Using software prefetching to load data that is soon to be accessed, thus getting it into the cache during cycles of other activity, can be very beneficial. This is because when the data is accessed "for real", the cache won't miss.

>Explicit prefetching done by the program is known as **software prefetching**. It should be noted that modern hardware architectures also have advanced **hardware prefetching** built in that can do an excellent automatic prefetching job if any memory access pattern is regular and predictable enough.

One example of when intelligent prefetching improves garbage collection speed in JRockit stems from the fact that the thread local areas for allocation are divided into many small chunks. When a chunk is first to be used, the next chunk is heuristically prefetched. This means that the following allocations will already have the next chunk of the TLA in the cache.

Done correctly, there are significant performance improvements in using prefetching to improve cache hits.

The downside is of course that every time an item is loaded into the cache, other cached data is destroyed. Prefetching too often may decrease cache functionality. Also, a prefetch retrieves an entire cache line, which takes time, so unless the prefetch operation can be **pipelined**, or hidden in parallel with other actions before its data is to be used, it has no or even negative effects.

<a name="3.4.3.2" />
### 3.4.3.2 Data placement

If we know certain data accesses to be sequential or close in time, it makes a lot of sense for the GC to try to place the objects involved on the same cache line—spatial locality follows temporal locality. An example is that a java.lang.String and the char array which it contains are almost always accessed in sequence. The more runtime feedback the memory system has, the better guesses it can make which data should belong together.

There are of course good static guesses as well, that usually pay off, such as trying to best-fit an object on the heap next to other objects that it may reference and an array next to its elements.

<a name="3.4.4" />
## 3.4.4 NUMA

Modern **Non-Uniform Memory Access (NUMA)** architectures provide even more challenges for a garbage collector. Typically, in a NUMA architecture, the address space is divided between processors. This is in order to avoid the bus (and possibly cache) latency bottleneck when several CPUs try to access the same memory. Each CPU owns a part of the address space and memory on a CPU-specific bus. A CPU that wants to access its own memory handles this very quickly, while the further away that the memory is that it wants to access, the longer it takes (depending on the configuration). The traditional approach is **Uniform Memory Access (UMA)**, where all CPUs uniformly access memory and where access time to a particular memory location is independent of which CPU requests it.

Two of the modern NUMA architectures that are used in several server-side environments are the AMD Opteron and the newer Intel Nehalem architecture.

The following figure illustrates a NUMA node configuration:

![Figure 3-11 "NUMA"][3]

Here, any CPU, or NUMA node, has to perform at most two communication hops with the other NUMA nodes in the system to get at arbitrary memory. The ideal (and the fastest) memory access consists of zero hops (the node's own memory). There is not necessarily be a one-to-one mapping between CPUs and NUMA nodes. For example, one NUMA node may contain several CPUs who share local memory.

So, to perform well on a NUMA architecture, the garbage collector threads should be structured in a beneficial way. If a CPU is executing a mark thread in the GC it should be the one working on the parts of the heap memory that belong to the CPU itself. This way NUMA performance is maximized. As referenced objects may, in worst case, appear anywhere on the heap, on NUMA the GC usually needs an additional object moving heuristic. This is to make sure that objects referenced near other objects in time appear near them in memory as well, evacuating them from suboptimal NUMA nodes. If this works correctly, there are substantial performance gains to be had. The main problem is keeping objects from being moved back and forth, "ping ponging", between memory sections that are the responsibilities of different NUMA nodes. Theoretically, an adaptive runtime could be very good at this.

This is another example of an optimization that can work well in an adaptive runtime, but perhaps not so much in a static environment. Command-line flags that modify memory allocation behavior in the JVM and change NUMA node affinity for the JVM process will be discussed in greater detail in [Chapter 5][1].

>NUMA is a challenging architecture to implement good memory management for. However, research on JRockit shows that it is still possible to get pretty good performance without specific NUMA optimizations as long as prefetching and cache behavior is intelligent enough in the JVM.

<a name="3.4.5" />
## 3.4.5 Large pages

At the base of all memory allocations lies the operating system and its page table. An OS divides the physical memory into pages, a page typically being the smallest possible memory allocation unit. Traditionally, a page is somewhere in the order of 4 KB. A process in an OS only sees a virtual address space, not a physical one. In order for a CPU to map a virtual page to the actual physical page in memory, a cache called the **Translation Lookaside Buffer (TLB)** is used to speed things up. If pages are too small, TLB misses are consequently more common.

This problem can be remedied if pages were several orders of magnitude larger; megabytes instead of kilobytes. All modern operating systems tend to support **large pages** in some form.

Obviously, in an OS where many processes allocate memory in separate address spaces and where pages are much larger than a couple of KB, fragmentation becomes a bigger problem because more page space is wasted. An allocation that requires slightly more memory than the size of a page suddenly carries a lot of dead weight. This doesn't matter to a runtime that does its own memory management in one process and owns a large part of the memory, but even if it were a problem it could be remedied by providing abstraction for many different page sizes on an underlying large page.

>A performance increase of at least 10 percent can usually be gained for a memory intensive application if it runs on large pages instead of normal ones. JRockit has support for this and can use large pages if enabled on the underlying operating system.

Typically, on most operating systems, enabling large page support is a privileged operation that requires administrator access, which makes it slightly harder to just "plug and play".

<a name="3.4.6" />
## 3.4.6 Adaptability

As we have discussed to a great extent in the chapter on code generation, adaptability is the key to success in a runtime for a mobile language such as Java. Traditionally, only code was adaptively reoptimized and was subject to hotspot analysis. However, the JRockit designers recognized from the start that all aspects of the runtime system should be made adaptive if possible.

So, JRockit may heuristically change garbage collection behavior at runtime, based on feedback from the memory system and adjust parameters such as heap size, number of generations in the heap, and even the overall strategy used to garbage collect.

Here is an example output generated by running JRockit with the `-Xverbose:gc` flag:

```
    marcusl@nyarlathotep:$ java -Xmx1024M -Xms1024M -Xverbose:gc -cp dist/bmbm.jar com.oracle.jrpg.bmbm.minisjas.server.Server
    [memory] Running with 32 bit heap and compressed references.
    [memory] GC mode: Garbage collection optimized for throughput, initial strategy: Generational Parallel Mark & Sweep.
    [memory] Heap size: 1048576KB, maximal heap size: 1048576KB, nursery size: 524288KB.
    [memory] <s>-<end>: GC <before>KB-><after>KB (<heap>KB), <pause>ms.
    [memory] <s/start> - start time of collection (seconds since jvm start).
    [memory] <end>     - end time of collection (seconds since jvm start).
    [memory] <before>  - memory used by objects before collection (KB).
    [memory] <after>   - memory used by objects after collection (KB).
    [memory] <heap>    - size of heap after collection (KB).
    [memory] <pause>   - total sum of pauses during collection (milliseconds).
    [memory]             run with -Xverbose:gcpause to see individual pauses.
	[memory] [YC#1] 28.298-28.431: YC 831035KB->449198KB (1048576KB), 132.7 ms
	[memory] [OC#1] 32.142-32.182: OC 978105KB->83709KB (1048576KB), 40.9 ms
	[memory] [OC#2] Changing GC strategy to Parallel Mark & Sweep
    [memory] [OC#2] 39.103-39.177: OC 1044486KB->146959KB (1048576KB), 73.0 ms
    [memory] [OC#3] Changing GC strategy to Generational Parallel Mark & Sweep
    [memory] [OC#3] 45.433-45.495: OC 1048576KB->146996KB (1048576KB), 61.8 ms
    [memory] [YC#2] 50.547-50.671: YC 968200KB->644988KB (1048576KB), 124.4 ms
    [memory] [OC#4] 51.504-51.524: OC 785815KB->21012KB (1048576KB), 20.2 ms
    [memory] [YC#3] 56.230-56.338: YC 741361KB->413781KB (1048576KB), 108.2 ms
    ...
    [memory] [YC#8] 87.853-87.972: YC 867172KB->505900KB (1048576KB), 119.4 ms
    [memory] [OC#9] 90.206-90.234: OC 875693KB->67591KB (1048576KB), 27.4 ms
    [memory] [YC#9] 95.532-95.665: YC 954972KB->591713KB (1048576KB), 133.2 ms
    [memory] [OC#10] 96.740-96.757: OC 746168KB->29846KB (1048576KB), 17.8 ms
    [memory] [YC#10] 101.498-101.617: YC 823790KB->466860KB (1048576KB), 118.8 ms
    [memory] [OC#11] 104.832-104.866: OC 1000505KB->94669KB (1048576KB), 34.5 ms
    [memory] [OC#12] Changing GC strategy to Parallel Mark & Sweep
    [memory] [OC#12] 110.680-110.742: OC 1027768KB->151658KB (1048576KB), 61.9 ms
    [memory] [OC#13] Changing GC strategy to Generational Parallel Mark & Sweep
    [memory] [OC#13] 116.236-116.296: OC 1048576KB->163430KB (1048576KB), 59.1 ms.
    [memory] [YC#11] 121.084-121.205: YC 944063KB->623389KB (1048576KB), 120.1 ms
```

>JRockit versions from R28 tend not to change garbage collection strategies at runtime. Default values will be picked depending on configuration. This, along with better garbage collectors, was found to provide a larger degree of determinism for customers.
>
>The previous output is from the R27 line of JRockit releases. For R28, non-standard GC strategies should be explicitly specified on the command line. R28 defaults to a generational parallel mark and sweep (optimized for throughput). The R28 memory management system still adaptively modifies many aspects of the garbage collection behavior, but to a lesser extent than R27.

All garbage collections in the previous example take place using a parallel mark and sweep algorithm, optimized for throughput. However, the JVM heuristically decides whether nurseries should be used or not depending on feedback from the runtime system. In the beginning, these changes are fairly frequent, but after a warm-up period and maintained steady-state behavior, the idea is that the JVM should settle upon an optimal algorithm. If, after a while, the steady-state behavior changes from one kind to another, the JVM may once again change strategies to a more optimal one.




[1]:    ../chap5/5.md
[2]:    ../images/3-10.jpg
[3]:    ../images/3-11.jpg
[4]:    http://docs.oracle.com/javase/6/docs/technotes/guides/jni/
[5]:    https://blogs.oracle.com/jrockit/entry/understanding_compressed_refer