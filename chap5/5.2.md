<a name="5.2" />
# 5.2 如何构建基准测试

如果不清楚应用程序的行为，就无法创建有效的基准测试，无论大小，都不能。要想弄清哪些基准测试与应用程序性能相关，就需要先对应用程序做全面的分析。

有不少工具有用来检测Java应用程序，有些通过在修改字节码来创建一个应用程序的特别版本来检测，有些则可以在不修改原有程序的基础上进行在线分析，JRockit Mission Control套件使用的是后者的方法。在[第6章][1]会对JRockit Mission Control做详细介绍。

对应用程序详细分析可以揭示出运行时把时间都花在哪些方法上了，垃圾回收工作是什么运行模式，以及哪些锁竞争激烈，哪些锁没什么竞争等信息。

其实，对应用程序进行分析并不一定非得要用什么神奇的工具，一切从简的话，直接用`System.out.println`方法在控制台打印出相关信息即可。

当收集到了足够的信息后，可以开始将应用程序划分为具体的子程序以分别进行基准测试。在创建基准测试之前，还需要仔细确认一下选定的子程序和基准测试是否针对同一个性能问题。

在对应用程序正式进行基准测试之前，要注意先让应用程序 **热身（warm-up）**，以使其达到稳定运行的状态。此外，思考以下问题：

* 为了简便起见，是否可以将应用程序调整为一个具有较短启动时间的、自包含的基准测试程序？
* 如果基准测试时间从1小时缩水到5分钟（这其中还包括了热身时间），那么这种比较还有意义吗，或者说，测试时间缩水的话能得到正确的测试结果吗？
* 基准测试是否会改变应用程序原有的行为？

>服务器端应用程序通常有多个功能子模块，适合对每个特定领域做基准测试。

理想的基准测试是一个模拟运行应用程序某一部分的、自包含的小程序。如果某个目标应用程序不易安装，而且要处理的输入数据太多，因而不易编写基准测试程序，那么可以尝试继续细分该应用程序，将之分解为一些可以处理有限输入数据的 **黑盒**，然后对这些 **黑盒**做基准测试，以此为基准对整个应用程序做出判断。

<a name="5.2.1" />
## 5.2.1 置身事外（outside the system）

除了一些非常小的基准测试和或验证某些概念的代码外，在做基准测试时，最好能够 **"置身事外"**。所谓 **"置身事外"**是指通过一些 **外部驱动程序（external driver）**来运行基准测试。在测试系统性能时，驱动程序独立于基准测试代码之外运行。

驱动程序通常会增加基准测试的工作量，例如可能会增加网络传输开销，因此如果基准测试要测量应用程序的响应时间，则得到的测试结果中会包括这部分通信时间。

使用驱动程序的好处是可以精确测量应用程序的响应时间，而受到数据生成或工作负载的影响。此外，为了确保数据生成或工作负载不会成为基准测试中的瓶颈，可以使驱动程序保持在较低的工作负载下运行。

下面的示例代码中，使用随机数据测试MD5算法，这个例子很好的说明了为什么执行基准测试时要 **置身事外**。

    import java.util.Random;
    import java.security.*;
    
    public class Md5ThruPut {
        static MessageDigest algorithm;
        static Random r = new Random();
        static int ops;
        
        public static void main(String args[]) throws Exception {
            algorithm = MessageDigest.getInstance("MD5");
            algorithm.reset();
            long t0 = System.currentTimeMillis();
            test(100000);
            long t1 = System.currentTimeMillis();
            System.out.println((long)ops / (t1 - t0) + " ops/ms");
        }

        public static void test(int size) {
            for (int i = 0; i < size; i++) {
                byte b[] = new byte[1024];
                r.nextBytes(b);
                digest(b);
            }
        }
        
        public static void digest(byte [] data) {
            algorithm.update(data);
            algorithm.digest();
            ops++;
        }
    }

如果基准测试的目标是衡量MD5算法的性能，那么上面的测试示例就可算是个反面教材了，由于生产随机数据的时间也被统计在内，所以基准测试的结果反映的是MD5算法和随机数生成算法两者结合之后的性能。虽然这是可能无心的，但却使测试结果不再可靠。下面是更加合理的基准测试代码：

    import java.util.Random;
    import java.security.*;

    public class Md5ThruPutBetter {
        static MessageDigest algorithm;
        static Random r = new Random();
        static int ops;
        static byte[][] input;
        
        public static void main(String args[]) throws Exception {
            algorithm = MessageDigest.getInstance("MD5");
            algorithm.reset();
            generateInput(100000);
            long t0 = System.currentTimeMillis();
            test();
            long t1 = System.currentTimeMillis();
            System.out.println((long)ops / (t1 - t0) + " ops/ms");
        }
        
        public static void generateInput(int size) {
            input = new byte[size];
            for (int i = 0; i < size; i++) {
                input[i] = new byte[1024];
                r.nextBytes(input[i]);
            }
        }
        
        public static void test() {
            for (int i = 0; i < input.length; i++) {
                digest(input[i]);
            }
        }
        
        public static void digest(byte [] data) {
            algorithm.update(data);
            algorithm.digest();
            ops++;
        }
    }

<a name="5.2.2" />
## 5.2.2 测量时间

在根据基准测试的结果作出结论之前，对应用程序的各个时间指标进行统计是非常重要的。最简便的方法是重复多次执行测试程序，以得到多次测量结果的标准差（standard deviation），只有当标准差在预定范围之内时，基准测试的结果才是真实有效的。

如果可能的话，应尽可能在多个同类机器上多次运行基准测试程序，这样有助于发现无心的配置错误，例如忘记了配置负载生成器，结果导致基准测试的结果较低等等。如果所有的基准测试都在同一台机器上执行，就难以发现这种因配置而产生的错误。

<a name="5.2.3" />
## 5.2.3 微基准测试（micro benchmark）

Micro benchmarks are benchmarks that contain a very small amount of code
and only measure a small piece of functionality, for example, how quickly the
JVM multiplies instances of  java.math.BigInteger or how quickly it does AES
encryption. Micro benchmarks are simple to write and often require just a single
or a few function calls that contain the algorithmic payload.



>Micro benchmarks are convenient, as they are simple to write and
simple to run. They may prove invaluable in understanding a particular
bottleneck in a very large application. They form an excellent backbone
both for regression testing against performance loss and for optimizing
the execution of known problems in the code, as discussed in the first
section of this chapter.
>We strongly encourage any application developer to keep a number of
micro benchmarks around for regression testing, the same way as we
encourage creating and keeping unit tests around when fixing bugs.
In both cases, this is to verify that a resolved issue doesn't ever happen
again and causes a regression.

If any large application could be reduced to a number of micro benchmarks, or at
least "mini benchmarks", life would be easier. Sadly, for modern complex applications
this is rarely the case. However, almost always, a fair (but for industrial purposes,
incomplete) amount of micro benchmarks can be created from the understanding
gained by profiling an application and figuring out what it does. The following code is
a function from a real world micro benchmark that is used as a performance regression
test for the JRockit JVM:

    public Result testArrayClear(Loop loop, boolean validate) {
        long count = 0;
        OpsPerMillis t = new OpsPerMillis("testArrayClear");
        t.start();
        loop.start();
        while (!loop.done()) {
            int[] intArray = new int[ARRAYSIZE];
            System.arraycopy(this.sourceIntArray, 0, intArray, 0, ARRAYSIZE);
            //Introduce side effects:
            //This call prevents dead code elimination
            //from removing the entire allocation.
            escape(intArray);
            count++;
        }
        t.end();
        return new OpsPerMillis(count, t.elapsed());
    }

Java requires that objects are cleared on allocation, so that all their fields are initiated
to their default values. The code optimizer in JRockit should be able to detect that
the newly allocated array  intArray is immediately and completely overwritten by
data, and so, as it is non-volatile, need not be cleared. If, for some reason, this code
optimization starts to fail, this benchmark will take longer to complete, and the QA
infrastructure will trigger a warning while examining the result database.

Micro benchmarks can (and should) also be created from scratch for problems
that are known to be performance-critical in an application. For example for an
XML parser, it makes sense to create a small benchmark that operates on a set of
auto-generated XML files of different sizes. For a math package that wants to use
java.math.BigDecimal , it makes sense to write a couple of small self-contained
applications that operate on  BigDecimal instances.

Creating micro benchmarks that aren't valid or that don't produce useful results
for the problem set is not just a waste of time and effort, but it is also potentially
harmful if the benchmark is believed to accurately measure all important aspects
of a problem. For example, testing an implementation of  java.util.HashMap by
just creating a  HashMap and filling it up with data might not be good enough. How
long does rehashing take? Extracting elements? What about collisions in  HashMap s
of different sizes?

Similarly, testing a  java.math.BigDecimal implementation by just performing a
large number of additions is almost certainly not good enough. What if there is a
fatal performance flaw in the division algorithm?

>When creating a micro benchmark, the main rule is always to
understand what you are measuring. Verify that the benchmark
is valid and that the result is useful.

While the previous two examples might seem somewhat artificial, they are
still examples of the kind of thinking that can lead you astray when creating
benchmarks. A somewhat more relevant example might be the case of
benchmarking an important synchronized operation in a class library. If the lock
in the synchronized operation is contended in an application, this obviously won't
show up in a single threaded micro benchmark. It may seem trivial that reducing the
load from many to fewer threads fundamentally changes the lock behavior, but this
is one of the main reasons that benchmarks fail to represent the real workload. Make
sure that any lock in a benchmark is actually stressed by many threads, if this is the
case in the real application.

Finally, it might make sense to try to eliminate parts of the runtime that aren't relevant
to the problem from the benchmark. If you want to measure pure code performance
for some algorithm, it probably makes little sense to create large numbers of objects
and stress the garbage collector at the same time. Or at least, pick a good garbage
collection strategy that won't interfere too much with the execution of the algorithm
(large heap, no nursery, and optimized for throughput)








[1]:    ../chap6/6.md#6