<a name="5.4" />
# 5.4 工业标准的基准测试

这些年来，工业界与学术界一直在努力在基准测试中模拟出Java应用程序可能会遇到的各种问题，对此，JVM厂商和硬件厂商当然是举双手欢迎，因为这不但可以提升JVM的运行效率，还有利于产品推广，标准化的基准测试通常会设计出与JVM调优相关的一些细节。为了能够更好的理解在不同的场景下应如何使用JVM，建议开发人员查看一下针对相关问题的基准测试报告。

自然地，基准测试的对象延展到了与编程相关的方方面面，很多软件栈都旨在提供标准化性能测量，测量对象无所不包，从应用程序服务器到网络库等等。对于Java开发人员来说，选择并使用相关的基准测试也算是一种修炼了。

>本节会以JVM位中心对基准测试进行介绍。了解如何JVM，以及如何做相关配置是很有意思的，所以本节中选用JVM厂商常用的几种基准测试套件做重点介绍。在之前的章节中已经介绍了优化JVM对不同类型应用程序的影响，良好的基准测试可以准确反映出应用程序在实际运行时的执行性能，
>
>本节所提到的一些基准测试套件，例如SPECjAppServer，同样可以作为通用基准测试应用于更大型的软件栈。

<a name="5.4.1" />
## 5.4.1 SPEC基准测试套件

[**SPEC（Standard Performance Evaluation Corporation）**][1]是一个非盈利性组织，其开发并维护着可用于对运行在现代硬件架构上的各类应用程序进行性能测试的多种基准测试套件。本节将对其中与Java相关的几种基准测试套件进行介绍。

除了本节中提到的几种SPEC基准测试套件和SPECjvm2008套件之外，其他的SPEC基准测试套件都是需要付费的。

<a name="5.4.1.1" />
### 5.4.1.1 SPECjvm基准测试套件

SPECjvm—its first incarnation released in 1998 as SPECjvm98 (now retired)—was
designed to measure the performance of a JVM/Java Runtime Environment. The
original SPECjvm98 benchmark contained almost only single-threaded, CPU-bound
benchmarks, which indeed said something about the quality of the code optimizations
in a JVM, but little else. Object working sets were quickly, after a few years, deemed
too small for modern JVMs. SPECjvm98 contained simple problem sets such as
compression, mp3 decoding, and measuring the performance of the  javac compiler.



The current version of SPECjvm, SPECjvm2008, is a modification of the original
SPECjvm98 suite, including several new benchmarks, updated (larger) workloads,
and it also factors in multi-core aspects. Furthermore, it tests out of the box settings
for the JVM such as startup time and lock performance.

Several real-life applications have recently been added to SPECjvm, such as the Java
database Derby, XML processors, and cryptography frameworks. Larger emphasis
than before has been placed on warm-up rounds and on measuring performance
from a steady state.

The venerable scientific computing benchmark SciMark has also been integrated into
SPECjvm2008. The original standalone version of SciMark suffered from the on-stack
replacement problem in that the  main function contained the main work loop for
each individual benchmark, which compromised, for example, the JRockit JVM and
made it hard to compare its results with those of other virtual machines. This has
been fixed in the SPECjvm2008 implementation.

<a name="5.4.1.2" />
### 5.4.1.2 SPECjAppServer套件与SPECjEnterprise2010套件

SPECjAppServer is a rather complex benchmark, and a rather good one, though hard
to set up. It started its life called ECPerf and has gone through several generations—
SPECjAppServer2001, SPECjAppServer2002, and SPECjAppServer2004. The latest
version of this benchmark has changed names to SPECjEnterprise2010, but the basic
benchmark setup is the same.

The idea behind this benchmark is to exercise as much of the underlying infrastructure,
hardware, and software, as possible, while running a typical J2EE application.
The J2EE application emulates a number of car dealerships interacting with a
manufacturer. The dealers use simulated web browsers to talk to the manufacturer,
and stock and transactions are updated and kept in a database. The manufacturing
process is implemented using RMI. SPECjEnterprise2010 has further modernized the
benchmark by introducing web services and more Java EE 5.0 functionality.

The SPECjAppServer suite is not just a JVM benchmark but it can also be used to
measure performance in everything from server hardware and network switches to
a particular brand of an application server. The publication guidelines specify that
the complete stack of software and hardware needs to be provided along with the
score. SPECjAppServer / SPECjEnterprise2010 is an excellent benchmark in that any
part of a complete system setup can be measured against a reference implementation.
This makes it relevant for everyone from hardware vendors to application server
developers. The benchmark attempts to measure performance of the middle tier of the
J2EE application, rather than the database or the data generator (driver).

The benchmark is quite complicated to set up and hard to tune. However, theoretically,
once set up, it can run self-contained on a single machine, but is rather resource heavy,
and this will not produce any interesting results.

A typical setup requires a **System Under Test (SUT)**, consisting of network
infrastructure, application servers, and a database server, all residing on different
machines. A driver, external to the test system, injects load into the setup. This is
an example of the technique for measuring outside the system that was explained
earlier in this chapter. In SPECjEnterprise2010, one of the more fundamental changes,
compared to earlier versions, is that database load has been significantly reduced so
that other parts of the setup (the actual application on the application server) becomes
more relevant for performance.

In a complete setup, the performance of everything, from which network switch to
which RAID solution the disk uses, matters. For a JVM, SPECjAppServer is quite a
good benchmark. It covers a large Java code base and the execution profile is spread
out over many long stack traces with no particular "extra hot" methods. This places
strict demands on the JIT to do correct inlining. It can no longer look for just a few
bottleneck methods and optimize them.

![Figure 4-10][1]

As of SPECjAppServer2004, in order to successfully run the benchmark, a transaction
rate (TxRate) is used for work packet injection. This is increased as long as the
benchmark can keep up with the load, and as soon as the benchmark fails, the
maximum TxRate for the benchmark system can thus be determined. This is used
to compute the score.

The realism is much better in newer generations of the benchmark. Application
servers use more up-to-date standards and workloads have been increased to fit
modern architectures. Support for multiple application servers and multiple driver
machines has also been added. Measurements have also been changed to more
accurately reflect performance.

<a name="5.4.1.3" />
### 5.4.1.3 SPECjbb套件

SPECjbb is probably one of the most widespread Java benchmarks in use today.
It is interesting because it has been used frequently in academic research, and has
been a point of competition between the big three JVM providers, Oracle, IBM,
and Sun Microsystems. The big three, in cooperation with hardware vendors,
have taken turns publishing press releases announcing new world records.

SPECjbb has existed in two generations, SPECjbb2000 (retired), and lately
SPECjbb2005, that is still in use. SPECjbb, similar to SPECjAppServer, emulates
a transaction processing system in several tiers, but is run on one machine in a
self-contained application.

SPECjbb has done the Java world some good in that a lot of optimizations that
JVMs perform, especially code optimizations, have been driven by this benchmark,
producing beneficial spin-off effects. There are also many examples of real-life
applications that the quest for SPECjbb scores has contributed performance
to—for example, more efficient garbage collection and locks.

>Here are some examples of functionality and optimizations
in the JRockit JVM that are a direct result of trying to achieve
high scores on SPECjbb. There are many more. All of these
optimizations have produced measurable performance
increases for real-world applications, outside the benchmark:
* Lazy unlocking (biased locking)
* Better heuristics for object prefetching
* Support for large pages for code and heap
* Support for non-contiguous heaps
* Improvements to array handling such as System.
arraycopy implementations, clear-on-alloc
optimizations, and assignments to arrays
* Advanced escape analysis that runs only on parts
of methods

A downside of SPECjbb is that it is, in fact, quite hardware-dependent. SPECjbb is very
much memory bound, and just switching architectures to one with a larger L2 cache
will dramatically increase performance.

Another downside to SPECjbb is that it is possible to get away with only caring
about throughput performance. The best scores can be achieved if all execution is
occasionally stopped and then, massive amounts of parallel garbage collection is
allowed to take place for as long as it takes to clean the heap.

SPECjbb2005 has also been used as the basis for the SPECpower_ssj2008 benchmark
that utilizes the same transaction code, but with an externalized driver. It is used
to quantify transactions per Watt at different levels of load, to provide a basis for
measuring power consumption.

>Here's another benchmarking anecdote. Sometimes an optimization
designed for a benchmark is only that and has no real world applications.
An example from JRockit is calls to System.currentTimeMillis.
This is a Java method that goes to native and finds out what the system
time is, expressed in milliseconds since January 1, 1970. Because this
typically involves a system call or a privileged operation, very frequent
calls to System.currentTimeMillis could be a bottleneck on several
platforms.
>Irritatingly enough, it turned out that calls to System.
currentTimeMillis made up several percent of the total runtime of
SPECjbb2000. On some platforms, such as Windows and Solaris, quick
workarounds to obtain system time were available, but not on Linux.
On Linux, JRockit got around the bottleneck by using its own signal-
based OS timers instead. The Linux JVM uses a dedicated thread to catch
an OS-generated signal every 10 milliseconds. Each time the signal is
caught, a local time counter is increased. This makes timer granularity a
little worse than with the system timer. However, as long as the timer is
safe (that is, it cannot go backwards) this maintains Java semantics and
SPECjbb runs much faster.
>Today on JRockit versions for Linux, native-safe timers are disabled.
If you, for some weird reason, have problems with the performance of
System.currentTimeMillis in your application, they can still be
enabled with the hidden flag -XX:UseSafeTimer=true. We have
never heard of anyone who needs this.






[1]:    /images/4-10.jpg
[2]:    http://www.spec.org