# 4.4 对于线程与同步的优化

本节将会对自适应运行时环境中线程和同步操作方面的优化进行介绍。

<a name="4.4.1" />
## 4.4.1 锁膨胀与锁收缩

正如之前介绍锁时提到的，自适应运行时系统需要能够当前系统负载和线程竞争情况在胖锁和瘦锁之间切换执行，这里就涉及到代码生成器和锁的具体实现。

在自适应环境中可以以很小的开销得到锁的运行信息（如果要做完整的锁分析则会有一些性能开销），当线程获取/释放某个锁时，运行时系统可以记录下是哪个线程获得了锁，获取锁时的竞争情况。所以，如果某个线程尝试了很多次还无法获取到锁，运行时就可以考虑将该瘦锁调整为胖锁。胖锁更适合竞争激烈的场景，不再让线程自旋，而挂起被阻塞住的线程，这样可以节省对CPU资源的浪费。这种瘦锁到胖锁的转换称为 **锁膨胀（lock inflation）**。

>默认情况下，即使瘦锁已经膨胀为胖锁，JRockit也是使用一个小的自旋锁来实现胖锁的。乍看之下，这不太符合常理，但这么做确实是很有益处的。如果锁的竞争确实非常激烈，而导致线程长时间自旋的话，可以使用命令行参数`-XX:UseFatSpin=false`可以禁用此方式。作为胖锁的一部分，自旋锁也可以利用到自适应运行时获取到的反馈信息，这部分功能默认是禁用的，可以使用命令行参数`-XX:UseAdaptiveFatSpin=true`来开启。

类似的，在完成了一系列解锁操作之后，如果锁队列和等待队列中都是空的，这时就可以考虑将胖锁再转换为瘦锁了，这称为 **锁收缩（lock deflation）**。

JRockit uses heuristics to perform both inflation and deflation, thus adapting to the
changed behavior of a given program where, for example, locks that were contended
in the beginning of program execution stop being contended. Then these locks are
candidates for deflation.

JRockit使用了启发式算法来执行锁膨胀和锁收缩，因此对于某个应用程序来说，锁的行为会根据线程对锁的竞争情况而改变。

The heuristics that trigger transitions between thin and fat locks can be overridden and
modified from the command line if needed, but this is generally not recommended.
The next chapter will briefly discuss how to do this.

如果需要的话，可以通过命令行参数来改变用于切换胖锁和瘦锁的启发式算法，但通常不建议这样做，[下一章][1]会对此做简要介绍。

<a name="4.4.2" />
## 4.4.2 递归锁

It is permissible, though unnecessary, for the same thread to lock the same object
several times, also known as recursive locking. Code that does so occurs, for
example, where inlining has taken place or in a recursive synchronized method.
Then the code generator may remove the inner locks completely, if no unsafe code
exists within the critical section, (such as volatile accesses or escaping calls between
the inner and outer locks).

This can be combined with optimizing for the recursive lock case. JRockit uses a
special lock token bit configuration to identify recursive locks. As long as a lock
has been taken at least twice by one thread without first being released, it is tagged
as recursive. So, forced unlock operations upon exceptions can still be correctly
implemented, resetting the recursion count to the correct state, with no extra
synchronization overhead.

<a name="4.4.3" />
## 4.4.3 锁融合

The JRockit optimizing JIT compiler also uses a code optimization called lock fusion
(sometimes also referred to as lock coarsening in literature). When inlining plenty of
code, especially synchronized methods, the observation can be made that frequent
sequential locks and unlocks with the same monitor object are common.

Consider code that, after inlining, looks like:

    synchronized(x) {
        //Do something...
    }
    
    //Short snippet of code...
    x = y;
    synchronized(y) {
        //Do something else...
    }

Classic alias analysis by the compiler trivially gives us that  x and  y are the same object.
If the short piece of code between the synchronized blocks carries little execution
overhead, less than the overhead of releasing and reacquiring the lock, it is beneficial
to let the code generator fuse the lock regions into one.

    synchronized(x) {
        //Do something...
        //Short snippet of code...
        x = y;
        //Do something else...
    }

Additional requirements are of course that there are no escaping or volatile
operations in the code between the synchronized blocks, or the Java Memory Model
semantics for equivalence would be violated. There are of course a few other code
optimization issues that have to be handled that are beyond the scope of this chapter.
An example would be that any exception handlers for the regions that are to be fused
need to be compatible.

Naturally, it might not be beneficial just to fuse every block of code we see, but we
can avoid some overhead if the blocks to fuse are picked cleverly. And if enough
sampling information is available for the short snippet of code, we can make clever
adaptive guesses to whether a lock fusion would be beneficial or not.

To summarize, this code optimization all boils down to not releasing a lock
unnecessarily. The thread system itself can, by making its state machine a little
bit more complicated, implement a similar optimization, independent of the code
generator, known as lazy unlocking.








[1]:    ../chap5/5.md