# 4.3 Java中线程与同步机制的实现

本节将对Java运行时中线程和同步机制的实现以及相关背景知识进行介绍，以便读者可以更好的处理并行结构，理解如何使用同步机制而不会造成较大的性能损耗。

<a name="4.3.1" />
## 4.3.1 Java内存模型

现在CPU架构中，普遍使用了数据缓存机制，可以大幅提升CPU对数据的读写速度，降低处理器总线的竞争程度。正如所有的缓存系统一样，这里也存在一致性问题，对于多处理器系统来说尤其重要，因为多个处理器有可能同时访问内存中同一位置的数据。

内存模型定义了不同的CPU在同时访问内存中同一位置是，何种情况下会看到相同的值，何种情况下会看到不同的值。强内存模型（例如x86平台上，就相当强）是指，当某个CPU修改了某个内存位置的值后，其他的CPU几乎自动就可以看到这个刚刚保存的值，在这种内存模型之下，内存写操作的执行顺序与代码中的排列顺序相同。弱内存模型（例如IA-64平台）是指，当某个CPU修改了某个内存位置的值后，其他的CPU不一定可以看到这个刚刚保存的值（除非CPU在执行写操作时附有特殊的内存屏障类指令），从更一般的角度说，所有由Java程序引起的内存访问都应该堆其他所有CPU可见，但事实上却不能保证一定立即可见。

不同硬件平台上对于"读-写" "写-读" "写-写"操作的处理有细微区别。Java为了屏蔽硬件区别，因此定义了JVM对这些操作的具体处理方式。对于像C++这样会编译为平台相关代码，而且缺少内存模型的的静态编程语言来说，在对操作读写时就会比较痛苦，需要仔细考虑不同平台之间的区别。尽管C++中也有`volatile`关键字，但应用程序的具体行为还无法独立于其所在的系统平台。此外，在C++中，部分"事实上的"内存模型并不属于编程语言自身，而是由线程库和操作系统调用决定的。在像Intel IA-64这种具有弱内粗模型的CPU架构上，程序员有时甚至会在代码中显式使用内存屏障。 总之，C++源代码在某个系统平台上完成编译，应用程序的行为也就不会再发生改变了。

Java是如何保证在所有支持的平台上具有相同的行为呢？在Java编程语言中并不存在内存屏障这东西，而且考虑到Java所追求的平台无关性，或许就不应该有内存屏障。

<a name="4.3.1.1" />
### 4.3.1.1 早期内存模型中的问题

之所以Java中需要有一个统一的内存模型，是为了保证在不同的硬件平台上能够有一致的行为。从Java的1.0版本到1.4版本，在实现内存模型的时候都是按照原始的Java语言规范做的。但是，第一版的Java内存模型中存在着严重的问题，实际使用时，会导致无法达到预期效果，而且有时还会使编译器的优化失效。

原始内存模型允许编译器对操作`volatile`变量和非`volatile`变量重新排序。以下面的代码为例：

    volatile int x;
    int y;
    volatile boolean finished;
    
    /* Code executed by Thread 1 */
    x = 17;
    y = 4711;
    finished = true;
    /* Thread 1 goes to sleep here */
    
    /* Code executed by Thread 2 */
    if (finished) {
        System.err.println(x):
        System.err.println(y);
    }

按照旧版的内存模型，上面的代码会打印出`17`，但未必会打印`4711`，这与`volatitle`关键字的语义有关。虽然`volatitle`关键字有明确的定义，但对非`volatitle`变量的读写操作却没有明确的定义。对那些常年与底层硬件打交道的程序员来说，这没什么，但大多数Java程序员可能认为，对变量`finished`的赋值操作应该会将其之前的赋值操作都刷入到内存中，其中包括非`volatitle`变量`y`。新的内存模型强制使用了更加严格的屏障来限制对`volatitle`变量和非`volatitle`变量的优化行为。

之前提到的无限循环的那个例子中，JIT编译器可能会为非`volatitle`变量在每个线程中创建一份局部拷贝。

考虑下面的代码：

    int operation(Item a, Item b) {
        return (a.value + b.value) * a.value;
    }

编译器可能会将上面的代码优化为下面的样子：

    int operation(Item a, Item b) {
        int tmp = a.value;
        return (tmp + b.value) * tmp;
    }

注意，上面的代码中，访问对`a.value`被合并为一次操作，在不同的CPU平台上，这种优化方式所所带来的性能提升不尽相同。但是，对JIT编译器来说，尽可能减少从内存载入数据的操作肯定是有好处的，因为访问内存会比访问寄存器慢上几个数量级。

原始的Java内存模型存在缺陷，因此若果编译器不能证明变量`a`和变量`b`是同一个对象的话，就不一定会执行上述优化。幸运的是，新版的Java内存模型中指定了只要定义属性域`value`时没有附加`volatitle`关键字，就可以执行上述优化。新版的内存模型允许线程为非`volatile`变量保存线程内拷贝，这也说明了[4.2.5][1]节中的示例代码为什么可能会产生无限循环。

<a name="4.3.1.1.1" />
#### 4.3.1.1.1 不变性

旧版Java内存模型的一大问题是，在某些情况下，被声明为`final`的变量是"可变的"。按照定义来说，被声明为`final`的变量因其不变性本来是不需要使用同步操作的，但在旧版内存模型中，却不是这样。在Java中，对被声明为`final`的成员变量的赋值只能在构造函数中进行，并且只能赋值一次，但在构造函数运行之前，未初始化的成员变量会有一个默认值，可以是0或者`null`。在旧版内存模型中，如果不显式使用同步操作的话，其他的线程可能在执行赋值操作将值提交到内存之前会看到这个默认值。

上述问题的典型事例就是`String`对象的使用。`String`类中所有成员变量都是加了`final`声明的，因此保证了`String`的实例是不可变对象，此外，为节省内存占用，多个`String`对象可能会使用一个共享字符数组来保存具体字符，并使用起始偏移位置和长度来定位具体的字符串。例如，字符串 **cat**和字符串 **housecat**就有可能共享同一个字符数组，按照旧版内存模型，在字符串 **cat**对象的构造函数执行之前，其起始偏移位置`offset`字段和长度`length`字段（默认值均为0）可能会被其他线程看到，使其认为该字符串对象的内容是"housecat"，当构造函数执行结束后，该字符串对象的内容又会被解读为"cat"，从实际效果行看，违反了"字符串是不可变的"这一原则。

>译者注：后来[`java.lang.String`][4]类的实现发生了变化（对应的发行版好像是jdk1.7.0_u6，但在ReleaseNote中没找到说明），抛弃了以往的"copy-on-write"模式，而是每个字符串对象都有自己的字符数组
>
>`String`[旧版实现][2]
>
>`String`[新版实现][3]
>
>另附String类的简单实现: [C++面试中string类的一种正确写法  by 陈硕][5]

新版本的内存模型修复了这个问题，声明为`final`的成员变量无需使用同步操作就可以保证器不变性。但需要注意的是，即便是使用`final`关键字来声明成员变量，但如果在构造函数完成之前就将该成员变量暴露给其他线程的话，仍然可能会出现上述问题。

>译者注：这里之所以仍可能会出问题，是因为按照Java语言规范（后续简称为JLS），[**happens-before**][6]规则只保证了在默认构造函数先于其他操作之前发生，但不保证会在其他操作之前结束，因此如果在构造函数结束之前就将当前对象暴露给其他线程，还是会出问题的。

<a name="4.3.1.2" />
### 4.3.1.2 JSR-133

重新设计Java内存模型是由JCP（[Java Comunity Process][7]组织完成的），具体内存在[JSR 133][8]（Java Specification Request 133）中，在2004年发布的Java 1.5中已成为Sun公司的参考实现。JSR以及JLS本身的内容都挺复杂，充满了精确的、形式化的描述内容。对JSR-133的详细介绍超出了本书的范畴，如果读者想要进一步提升自己的编程水平，详细阅读一下相关内容是很有帮助的。(译者注，相关内容参见[The Java Memory Model][9])

>网上有很多对JSR-133进行介绍的内容，例如由[Jeremy Manson][13]和[Brian Goetz][12]编写的[JSR-133 FAQ][10]，以及由[Brian Goetz][12]编写的[Java并发编程实践][11]

JSR-133解决了`volatitle`重排序的问题，明确了`final`关键字的语义，保证了其不变性，此外，还可以修复以往JDK版本中一些客剑心问题。`volatitle`的语义更严格，相应的，执行效率略有下降。

对于Java来说，JSR-133和新版的内存模型是一大进步，使 **同步**的语义更简单，更直观，直接使用`volatitle`关键字就可以实现简单的同步访问。当然，即便是有了新版的内存模型，在多线程环境下，操作内存仍然可能会出现各种问题，但以往那种因语义不明而导致的问题已经一去不复返了，因此，深入理解同步机制，明确锁和`volatitle`关键的用途才能生产出优质程代码。

<a name="4.3.2" />
## 4.3.2 实现同步

下面的内容将介绍Java字节码和JVM中是如何实现同步的。

<a name="4.3.2.1" />
### 4.3.2.1 原生机制

从计算机的最底层结构来说（例如CPU架构），同步是使用原子指令实现的，各个平台的具体实现可能有所不同，以x86平台为例，使用了专门的 **锁前缀（lock prefix）**（译者注，参见[Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 2][18] Vol. 2A 3-465中对锁前缀的说明）来实现多处理器环境中指令的原子性。

在大多数CPU架构中，标准指令（例如加法和减法指令）都可以实现为原子指令。

为了实现原子的条件载入和条件存储指令，比较常用的 [**比较-交换（compare and exchange）指令**][19]（译者注，参见参见[Intel® 64 and IA-32 Architectures Software Developer’s Manual Volume 2][18] Vol. 2A 3-148中对`cmpxchg`指令的说明）。**比较-交换**指令会对内存中指定位置和的值和一个期望值作比较，如果相同，则将另一个输入参数的值写入到内存的指定位置，并设置`ZF`寄存器（译者注：标记寄存器的内容参见[这里][20]）；如果不同，则清空`ZF`寄存器，返回期望的数值。由此该指令可以产生不同的执行结果，后文将会提到，**比较-交换**指令是实现锁的基础。

此外，[**内存屏障（memory fence）**][21]指令也可用来保证所有的CPU都可以看到对内存中数据的最新修改，因此可将之用来实现`volatitle`关键字的语义，编译器会在每个对`volatitle`变量赋值的语句后面，插入一条内存屏障指令，以便其他的CPU可以看到本次修改。

由于内存屏障强制执行内存有序，使CPU缓存失效，无法并行执行指令，因此使用原子指令是有性能损耗的，即使原子指令是实现同步的基础，在使用的时候也需要多加小心。

对调用原子指令的常用优化是将之作为JVM的 [**内建函数（intrinsic call）**][21]使用，例如，当运行时发现调用了`java.util.concurrent.atomic`包中某些函数，可以将之简单的实现为内联的汇编指令。以下面的代码为例：

    import java.util.concurrent.atomic.*;
    
    public class AtomicAdder {
        AtomicInteger counter = new AtomicInteger(17);
        public int add() {
            return counter.incrementAndGet();
        }
    }
    
    public class AtomicAdder {
        int counter = 17;
        public int add() {
            synchronized(this) {
                return ++counter;
            }
        }
    }

在第一个`AtomicAdder`类中，如果运行时能够识别出`add`方法具体功能，就无需关心`AtomicInteger.incrementAndGet`方法的具体实现，在生成代码时直接插入原子的加法指令即可。之所以可以用这种方法操作，是因为`java.util.concurrent.AtomicInteger`类是JDK的一部分，具有良好的语义定义。如果没有原子指令的话，虽然可也已实现相同的功能，但会更麻烦一些。

实践中，使用同步操作可以实现排他性访问，但失去并行性会带来不小的性能损耗，这是因为除了一次只有一个线程可以访问关键区之外，同步操作本身也会带来性能损耗。

在微架构（micro-architecture）层面，原子指令的执行在各个平台上不尽相同。一般情况下，它会暂停向CPU流水线添加新指令，直到所有已有的指令都完成执行，并将操作结果刷入到内存中。此外，该CPU还会阻止其他CPU对相关缓存行（cache line）的访问，直到该原子指令结束执行。在现代x86硬件平台上，如果屏障指令（fence instruction）中断了比较复杂的指令执行，则该原子指令可能需要等上很多个时钟周期才能开发执行。因此，不仅是过多的关键区会影响系统性能，锁的具体实现也会影响性能，而当频繁对较小的关键区执行加锁、解锁操作时，性能损耗更是巨大。

<a name="4.3.2.2" />
### 4.3.2.2 锁(Lock)

尽管操作系统提供的同步机制，实现锁的时候只需要将未得到锁的线程放入等待队列即可，但这种粗放的、一锅端的方式并不适用于所有场合。

如果某个锁从未有竞争出现，而且加锁的次数也非常有限，又或者锁的竞争非常激烈，这两种情况下，需要针对各自的应用场景选择合适的锁实现，这是自适应运行时的又一个用武之地。在介绍自适应运行时如何根据反馈信息选择锁实现之前，这里需要先介绍两种基本锁实现，即 **胖锁（fat lock）**和**瘦锁（thin lock）**。

瘦锁通常用于没什么竞争，且锁的持有时间很短的场景中，胖锁则用于更复杂一些的场景中。运行时可以根据锁的竞争情况，在两种类型之间做切换。

<a name="4.3.2.2.1" />
#### 4.3.2.2.1 Thin Lock

The simplest implementation of a thin lock is the **spinlock**. A spinlock spends its
time in a  while loop, waiting for its monitor object to be released—that is, burning
CPU cycles. Typically, a spinlock is implemented with an atomic compare and
exchange instruction to provide the basic exclusivity, and a conditional jump
back to the compare and exchange if the test failed to acquire the lock.

Following is the pseudocode for a very simple spinlock implementation:

    public class PseudoSpinlock {
        private static final int LOCK_FREE = 0;
        private static final int LOCK_TAKEN = 1;
        //memory position for lock, either free or taken
        static int lock;
        
        /**
         * try to atomically replace lock contents with
         * LOCK_TAKEN.
         *
         * cmpxchg returns the old value of [lock].
         * If lock already was taken, this is a no-op.
         *
         * As long as we fail to set the taken bit,
         * we spin
         */
        public void lock() {
            //burn cycles, or do a yield
            while (cmpxchg(LOCK_TAKEN, [lock]) == LOCK_TAKEN);
        }
        
        /**
         * atomically replace lock contents with "free".
         */
        public void unlock() {
            int old = cmpxchg(LOCK_FREE, [lock]);
            //guard against recursive locks, i.e. the same lock
            //being taken twice
            assert(old == LOCK_TAKEN);
        }
    }

Due to the simplicity and low overhead of entering a spinlock, but because of the
relatively high overhead maintaining it, spinlocks are only optimal if used in an
implementation where locks are taken for very short periods of time. Spinlocks do
not handle contention well. If the lock gets too contended, significant runtime will
be wasted executing the loop that tries to acquire the lock. The  `cmpxchg` itself is also
dangerous when frequently executed, in that it may ruin caches and prevent any
thread from running at maximum capacity.

Spinlocks are referred to as "thin" if they are simple to implement and take up
few resources in a contention free environment. Less intrusive varieties can be
implemented with slightly more complex logic (for example adding a yield or
CPU pause to the spin loop), but the basic idea is the same.

As the implementation is nothing but a  `while` loop with an atomic check, spinlocks
cannot be used to support every aspect of Java synchronization. One example is the
wait/notify mechanism that has to communicate with the thread system and the
scheduler in order to put threads to sleep and wake them up when so required.

<a name="4.3.2.2.2" />
#### 4.3.2.2.2 Fat Lock

Fat locks are normally an order of magnitude slower than thin locks to release or
acquire. They require a more complex representation than the thin lock and also have
to provide better performance in a contended environment. Fat lock implementations
may, for example, fall back to an OS level locking mechanism and thread controls.

Threads waiting for a fat lock are suspended. A **lock queue** for each fat lock is
typically maintained, where the threads waiting for the lock are kept. The threads
are usually woken up in FIFO order. The lock queue may be rearranged by the
runtime as the scheduler sees fit or based on thread priorities. For objects used in
`wait` /  `notify` constructs, the JVM may also keep a **wait queue** for each monitor
resource where the threads that are to be notified upon its release are queued.

<a name="4.3.2.2.3" />
#### 4.3.2.2.3 A word on fairness

In scheduling, the term **fairness** is often used to describe a scheduling policy where
each thread gets an equally sized time quantum to execute. If a thread has used its
quantum, another thread gets an opportunity to run.

If fairness is not an issue—such as when we don't need a certain level of even **thread
spread** over CPUs and threads perform the same kind of work—it is, in general, faster
to allow whatever thread that gets a chance to run to keep running. Simply put, if we
are just concerned about maximizing Java execution cycles, it can be a good idea to
let a thread that just released a lock reacquire it again. This avoids expensive context
switching and doesn't ruin the caches. Surprisingly enough, unfair behavior like this
from the scheduler can, in several cases, improve runtime performance.

When it comes to thin locks, there is actually no fairness involved by design.
All locking threads race with each other when attempting to acquire a lock.

With fat locks, in principle, the same thing applies. The lock queue is ordered, but
threads will still have to race for the lock if several threads at once are awoken from
the queue.

<a name="4.3.2.2.4" />
#### 4.3.2.2.4 JRockit中的锁字

Recollect the 2 x 32-bit word header of any object in JRockit. One word is the class
block that contains a pointer to type information for the object. The other word is the
lock and GC word. Of the 32 bits in the lock and GC word, JRockit uses 8 bits for GC
information and 24 bits for lock information.

>The lock word and object header layout described in this section
reflects the current state of the implementation in JRockit R28 and
is subject to change without notice between releases. The bit-level
details are only introduced to further help explaining lock states
and their implementation.

In JRockit, every lock is assumed to be a thin lock when first taken. The lock bits
of a thin locked object contain information about the thread that is holding the lock,
along with various extra information bits used for optimization. For example for
keeping track of the number of lock transfers between threads to determine if a
lock is mostly thread local, and thus mostly unnecessary.

A fat lock requires a JVM internal monitor to be allocated for lock and semaphore
queue management. Therefore, most of the space in the lock word for fat locks is
taken up by an index (handle) to the monitor structure.

![Figure 4-3][14]

The JRockit lock bits are one example of an implementation of helper data structures
for thin and fat locks, but of course both object header layout and contents vary
between different vendors' JVM implementations. The various state diagrams that
follow, go into some more detail on how thin locks and fat locks are converted to
each other in JRockit and how lock words are affected by the state transitions.

![Figure 4-4][15]

The previous figure shows the relatively simple transitions between a locked and
unlocked state if only thin locks are involved. An unlocked object is locked when
thread **T1** executes a successful lock on the object. The lock word in the object header
now contains the thread ID of the lock taker and the object is flagged as thin locked.
As soon as T1 executes an unlock, the object reverts back to unlocked state and the
lock holder bits are zeroed out in the object header.

![Figure 4-5][16]

If we add fat locks to the picture, things get a little bit more complex. Recollect that
a thin lock can be inflated into a fat lock if it is determined that it is too contended
or if a call to  wait is applied to it, such as a call to  `wait` . The object can go directly to
fat locked if thread **T1** attempts to acquire it and it is known to be contended. There
is also a path from the thin locked version of the lock to the fat locked version in the
event of a  wait call. As a fat lock keeps its allocated JVM internal monitor, along
with the handle to it, in the lock word, unlocking a fat lock without finding the
need to deflate it will retain the monitor ID in the lock word, reusing the
monitor structure.

The section on [Pitfalls and false optimizations][17] later in this chapter will further discuss
how the runtime adaptively turns thin locks and fat locks into one another using
contention based heuristics.









[1]:    ./4.2.md#4.2.5
[2]:    http://hg.openjdk.java.net/jdk7/jdk7/jdk/file/9b8c96f96a0f/src/share/classes/java/lang/String.java
[3]:    http://hg.openjdk.java.net/jdk7u/jdk7u/jdk/file/19cc3b567644/src/share/classes/java/lang/String.java
[4]:    http://hg.openjdk.java.net/jdk7u/jdk7u/jdk/rev/e1c679a00712
[5]:    http://coolshell.cn/articles/10478.html
[6]:    http://docs.oracle.com/javase/specs/jls/se7/html/jls-17.html#jls-17.4.5
[7]:    https://www.jcp.org/en/home/index
[8]:    https://www.jcp.org/en/jsr/detail?id=133
[9]:    http://www.cs.umd.edu/~pugh/java/memoryModel/
[10]:   http://www.cs.umd.edu/~pugh/java/memoryModel/jsr-133-faq.html
[11]:   http://www.amazon.cn/Java%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E6%88%98-%E7%9B%96%E8%8C%A8/dp/B0077K9XHW/ref=sr_1_1?ie=UTF8&qid=1391584769&sr=8-1&keywords=java+%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AE%9E%E8%B7%B5
[12]:   http://www.briangoetz.com/
[13]:   http://jeremymanson.blogspot.com/
[14]:   ../images/4-3.jpg
[15]:   ../images/4-4.jpg
[16]:   ../images/4-5.jpg
[17]:   ./4.5.md
[18]:   http://www.intel.com/content/dam/www/public/us/en/documents/manuals/64-ia-32-architectures-software-developer-instruction-set-reference-manual-325383.pdf
[19]:   http://en.wikipedia.org/wiki/Compare-and-swap
[20]:   http://caoxudong.info/post/the_intel_microprocessors_note_1
[21]:   http://en.wikipedia.org/wiki/Memory_barrier